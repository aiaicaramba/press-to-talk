{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3024e2",
   "metadata": {},
   "source": [
    "# Lesson 3: Reflection and Blogpost Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cc42f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ollama\n",
    "!python -m pip install openai\n",
    "!python -m pip install 'litellm[proxy]'\n",
    "## !python -m pip install openllama\n",
    "## !python -m pip install getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d39be0-eaf3-456d-8613-ba21099ed36b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "\n",
    "\n",
    "local = True\n",
    "if local:\n",
    "  llm_config={\n",
    "      \"config_list\": [\n",
    "          {\n",
    "              \"model\": \"NotRequired\", # Loaded with LiteLLM command\n",
    "              \"api_key\": \"NotRequired\", # Not needed\n",
    "              \"base_url\": \"http://0.0.0.0:4000\"  # Your LiteLLM URL\n",
    "          }\n",
    "      ],\n",
    "  }\n",
    "else:\n",
    "    llm_config = {\"model\": \"gpt-4o\"}\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a622d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use llama3 model locally via LiteLLM\n",
    "# docu:    https://microsoft.github.io/autogen/docs/topics/non-openai-models/local-litellm-ollama\n",
    "# Run in the vs code terminal:    (get the terminal with: ctrl+` )\n",
    "# litellm --model ollama_chat/llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969e6bb",
   "metadata": {},
   "source": [
    "## The task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT USED... only here at the moment as a cheat sheet\n",
    "from autogen import UserProxyAgent, ConversableAgent\n",
    "\n",
    "local_llm_config={\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"NotRequired\", # Loaded with LiteLLM command\n",
    "            \"api_key\": \"NotRequired\", # Not needed\n",
    "            \"base_url\": \"http://0.0.0.0:4000\"  # Your LiteLLM URL\n",
    "        }\n",
    "    ],\n",
    "    \"cache_seed\": None # Turns off caching, useful for testing different models\n",
    "}\n",
    "\n",
    "# Create the agent that uses the LLM.\n",
    "assistant = ConversableAgent(\"agent\", llm_config=local_llm_config)\n",
    "\n",
    "# Create the agent that represents the user in the conversation.\n",
    "user_proxy = UserProxyAgent(\"user\", code_execution_config=False)\n",
    "\n",
    "# Let the assistant start the conversation.  It will end when the user types exit.\n",
    "#assistant.initiate_chat(user_proxy, message=\"How can I help you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8074032-3690-4de9-ad08-ea8323cb441b",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "task = '''\n",
    "       Discuss a news item to help a Human Journalist write an engaging news article about a subject given by the User. \n",
    "       After discussing propose an article (including a headline).\n",
    "       Make sure the article is within 100 words. \n",
    "       Every article should have a headline, a lead, and a body.\n",
    "       Het artikel moet in het Nederlands zijn.\n",
    "       '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987f023",
   "metadata": {},
   "source": [
    "## Create 2 Journallist agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f0a47-a9fe-43a0-b7b1-79922e4c4ac8",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "seriousJournalist = ConversableAgent(\n",
    "    name=\"Serious Journalist\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a Serious Journalist. You discuss a news item \n",
    "                with another Agent, a Sensational Journalist, and you want \n",
    "                to make sure all facts are checked before an article is published. \n",
    "                Give feedback when it is possible to improve the quality of the content, \n",
    "                or when facts have to be checked.\n",
    "                De conversatie moet in het Nederlands zijn. \n",
    "                Als de Sensational Journalist wat snel conclusies wil trekken, \n",
    "                moet je hem/haar corrigeren. Laat gerust merken dat je het irritant \n",
    "                vindt als iemand te snel concllusies wil trekken.\"\"\",\n",
    "    human_input_mode=\"ALWAYS\"\n",
    ")\n",
    "\n",
    "sensationalJournalist = ConversableAgent(\n",
    "    name=\"Sensational Journalist\",\n",
    "    system_message=\"\"\"You are a Journalist. You write engaging news articles (with title) \n",
    "        on given topics. You like to make it Sensational, to make it appealing for the reader! \n",
    "        But you have to discuss \n",
    "        with another Agent, a Serious Journalist, who wants to be sure \n",
    "        all facts are checked. \"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b4d8d-40f7-4a05-8958-25d20054de3a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "reply = sensationalJournalist.generate_reply(messages=[{\"content\": task, \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49658114",
   "metadata": {},
   "source": [
    "## Adding reflection \n",
    "\n",
    "Create a critic agent to reflect on the work of the writer agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d5fdb-6081-470b-b287-8cf8b8142d0d",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "res = seriousJournalist.initiate_chat(\n",
    "    recipient=sensationalJournalist,\n",
    "    message=task,\n",
    "    max_turns=2,\n",
    "    summary_method=\"last_msg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b76449",
   "metadata": {},
   "source": [
    "## Nested chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ecf92-63e9-40ff-aeed-1c404352e4ab",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "facts_checker = ConversableAgent(\n",
    "    name=\"Facts Checker\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a reviewer, known for \n",
    "        your ability to check facts. \n",
    "        When you see a fact that could be incorrect, \n",
    "        you should point it out. \n",
    "        Then you think of a way to check if it is a fact. \n",
    "        If you can do it yourself: do it (and tell it), \n",
    "        otherwise tell the Journalists how to check it.\"\"\",\n",
    ")\n",
    "\n",
    "legal_reviewer = ConversableAgent(\n",
    "    name=\"Legal Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a legal reviewer, known for \"\n",
    "        \"your ability to ensure that content is legally compliant \"\n",
    "        \"and free from any potential legal issues. \"\n",
    "        \"Make sure your suggestion is concise (within 3 bullet points), \"\n",
    "        \"concrete and to the point. \"\n",
    "        \"Begin the review by stating your role.\",\n",
    ")\n",
    "\n",
    "ethics_reviewer = ConversableAgent(\n",
    "    name=\"Ethics Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are an ethics reviewer, known for \"\n",
    "        \"your ability to ensure that content is ethically sound \"\n",
    "        \"and free from any potential ethical issues. \" \n",
    "        \"Make sure your suggestion is concise (within 3 bullet points), \"\n",
    "        \"concrete and to the point. \"\n",
    "        \"Begin the review by stating your role. \",\n",
    ")\n",
    "\n",
    "meta_reviewer = ConversableAgent(\n",
    "    name=\"Meta Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a meta reviewer, you aggragate and review \"\n",
    "    \"the work of other reviewers and give a final suggestion on the content.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913beca1",
   "metadata": {},
   "source": [
    "## Orchestrate the nested chats to solve the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a70c7-19ca-4e5a-ad3d-f2b481fb5915",
   "metadata": {
    "height": 540
   },
   "outputs": [],
   "source": [
    "def reflection_message(recipient, messages, sender, config):\n",
    "    return f'''Discuss the following news item. \n",
    "            \\n\\n {recipient.chat_messages_for_summary(sender)[-1]['content']}'''\n",
    "\n",
    "review_chats = [\n",
    "    {\n",
    "     \"recipient\": facts_checker, \n",
    "     \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'Checker': '', 'FactCheck': ''}. Here Fact Checker should be your role\",},\n",
    "     \"max_turns\": 3},\n",
    "    {\n",
    "    \"recipient\": legal_reviewer, \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'Reviewer': '', 'Review': ''}.\",},\n",
    "     \"max_turns\": 1},\n",
    "    {\"recipient\": ethics_reviewer, \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'reviewer': '', 'review': ''}\",},\n",
    "     \"max_turns\": 1},\n",
    "     {\"recipient\": meta_reviewer, \n",
    "      \"message\": \"Aggregrate feedback from all reviewers and give final suggestions on the writing.\", \n",
    "     \"max_turns\": 1},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a40b66-5061-460d-ad9d-c0dbcfbba2e9",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "seriousJournalist.register_nested_chats(\n",
    "    review_chats,\n",
    "    trigger=sensationalJournalist,\n",
    ")\n",
    "\n",
    "res = seriousJournalist.initiate_chat(\n",
    "    recipient=sensationalJournalist,\n",
    "    message=task,\n",
    "    max_turns=2,\n",
    "    summary_method=\"last_msg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c833b0",
   "metadata": {},
   "source": [
    "## Get the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef82ed-f102-4964-b7be-60e2f258a39b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(res.summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
