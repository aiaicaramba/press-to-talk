{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3024e2",
   "metadata": {},
   "source": [
    "# Lesson 3: Reflection and Blogpost Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cc42f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ollama\n",
    "!python -m pip install openai\n",
    "!python -m pip install 'litellm[proxy]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d39be0-eaf3-456d-8613-ba21099ed36b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "\n",
    "local = True\n",
    "if local:\n",
    "  llm_config={\n",
    "      \"config_list\": [\n",
    "          {\n",
    "              \"model\": \"NotRequired\", # Loaded with LiteLLM command\n",
    "              \"api_key\": \"NotRequired\", # Not needed\n",
    "              \"base_url\": \"http://0.0.0.0:4000\"  # Your LiteLLM URL\n",
    "          }\n",
    "      ],\n",
    "      \"cache_seed\": None # Turns off caching, useful for testing different models\n",
    "  }\n",
    "else:\n",
    "    llm_config = {\"model\": \"gpt-4o\"}\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e47f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use llama3 model locally via LiteLLM\n",
    "# docu:    https://microsoft.github.io/autogen/docs/topics/non-openai-models/local-litellm-ollama\n",
    "# Run in the vs code terminal:    (get the terminal with: ctrl+` )\n",
    "# litellm --model ollama_chat/llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969e6bb",
   "metadata": {},
   "source": [
    "## The task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8074032-3690-4de9-ad08-ea8323cb441b",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "task = '''\n",
    "       To start with: Ask the user, who is a journalist, for a subject to write an article about.\n",
    "       Discuss that news item to help the Human Journalist write an engaging news article about that subject. \n",
    "       After discussing propose an article (including a headline). \n",
    "       Make sure the article is within 100 words. \n",
    "       Every article should have a headline, a lead, and a body. \n",
    "       Het artikel moet in het Nederlands zijn. \n",
    "       '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987f023",
   "metadata": {},
   "source": [
    "## Create Journalistic agents & user_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0f0a47-a9fe-43a0-b7b1-79922e4c4ac8",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "from autogen import UserProxyAgent, ConversableAgent\n",
    "\n",
    "seriousJournalist = ConversableAgent(\n",
    "    name=\"Serious Journalist\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Vraag aan de gebruiker om een onderwerp te kiezen voor een nieuwsartikel.\n",
    "                You are a Serious Journalist, who writes in Dutch. You discuss a news item \n",
    "                with another Agent, a Sensational Journalist, and you want \n",
    "                to make sure all facts are checked before an article is published. \n",
    "                Give feedback when it is possible to improve the quality of the content, \n",
    "                or when facts have to be checked.\n",
    "                De conversatie moet in het Nederlands zijn. \n",
    "                Als de Sensational Journalist wat snel conclusies wil trekken, \n",
    "                moet je hem/haar corrigeren. Laat gerust merken dat je het irritant \n",
    "                vindt als iemand te snel concllusies wil trekken.\"\"\",\n",
    "    human_input_mode=\"ALWAYS\"\n",
    ")\n",
    "\n",
    "sensationalJournalist = ConversableAgent(\n",
    "    name=\"Sensational Journalist\",\n",
    "    system_message=\"\"\"You are a Dutch Journalist. You write engaging news articles (with title) \n",
    "        on given topics, in Dutch. You like to make it Sensational, to make it appealing for the reader! \n",
    "        But you have to discuss \n",
    "        with another Agent, a Serious Journalist, who wants to be sure \n",
    "        all facts are checked. \"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# And an agent that represents the user in the conversation.\n",
    "user_proxy = UserProxyAgent(\"user\", code_execution_config=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49658114",
   "metadata": {},
   "source": [
    "## Let's chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899d5fdb-6081-470b-b287-8cf8b8142d0d",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ConversableAgent.register_nested_chats() missing 1 required positional argument: 'chat_queue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mseriousJournalist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_nested_chats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrigger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msensationalJournalist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m res \u001b[38;5;241m=\u001b[39m seriousJournalist\u001b[38;5;241m.\u001b[39minitiate_chat(\n\u001b[1;32m      6\u001b[0m     recipient\u001b[38;5;241m=\u001b[39muser_proxy,\n\u001b[1;32m      7\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaarover gaan we een artikel over schrijven?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     max_turns\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      9\u001b[0m     summary_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: ConversableAgent.register_nested_chats() missing 1 required positional argument: 'chat_queue'"
     ]
    }
   ],
   "source": [
    "seriousJournalist.register_nested_chats(\n",
    "    chat_queue=[sensationalJournalist,user_proxy],\n",
    "    trigger=sensationalJournalist,\n",
    ")\n",
    "\n",
    "res = seriousJournalist.initiate_chat(\n",
    "    recipient=user_proxy,\n",
    "    message=\"Waarover gaan we een artikel over schrijven?\",\n",
    "    max_turns=5,\n",
    "    summary_method=\"last_msg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913beca1",
   "metadata": {},
   "source": [
    "## Orchestrate the nested chats to solve the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a40b66-5061-460d-ad9d-c0dbcfbba2e9",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "seriousJournalist.register_nested_chats(\n",
    "    review_chats,\n",
    "    trigger=sensationalJournalist,\n",
    ")\n",
    "\n",
    "res = seriousJournalist.initiate_chat(\n",
    "    recipient=sensationalJournalist,\n",
    "    message=task,\n",
    "    max_turns=2,\n",
    "    summary_method=\"last_msg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c833b0",
   "metadata": {},
   "source": [
    "## Get the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef82ed-f102-4964-b7be-60e2f258a39b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(res.summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
