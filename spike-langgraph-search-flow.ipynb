{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike LangGraph Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq langchain langgraph langchain_openai langchainhub langsmith duckduckgo-search beautifulsoup4 gradio ipywidgets tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Commented out if set in .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'lm-studio' # Set to 'lm-studio' for local LMStudio inference\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = ''\n",
    "\n",
    "# Set LangSmith tracing and project name reference variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true' \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = '20240422-Spike LangGraph Local LLM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import functools, operator, requests, os, json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "#llm = ChatOpenAI(model=\"NA\", base_url=\"http://192.168.50.116:8000/v1\") # Use for local inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TavilySearchResults(),\n",
       " StructuredTool(name='process_content', description='process_content(url: str) -> str - Processes content from a webpage.', args_schema=<class 'pydantic.v1.main.process_contentSchema'>, func=<function process_content at 0xffff3aa7b8b0>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Internet Search Tool (TavilySearch)\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "internet_search = TavilySearchResults(max_results=5)\n",
    "\n",
    "# Process Content Tool (BS4)\n",
    "@tool(\"process_content\", return_direct=False)\n",
    "def process_content(url: str) -> str:\n",
    "    \"\"\"Processes content from a webpage.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Tool set\n",
    "tools = [internet_search, process_content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for creating agents\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor\n",
    "\n",
    "# Define agent nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Web_Searcher', 'Insight_Researcher']\", 'members': 'Web_Searcher, Insight_Researcher'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['members'], template=\"As a supervisor, your role is to oversee a dialogue between these workers: {members}. Based on the user's request, determine which worker should take the next action. Each worker is responsible for executing a specific task and reporting back their findings and progress. Once all tasks are complete, indicate with 'FINISH'.\")), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0xffff3b3cba30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0xffff3aa79730>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Web_Searcher', 'Insight_Researcher']}]}}, 'required': ['next']}}], 'function_call': {'name': 'route'}})\n",
       "| JsonOutputFunctionsParser()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Agent Supervisor\n",
    "members = [\"Web_Searcher\", \"Insight_Researcher\"]\n",
    "system_prompt = (\n",
    "    \"As a supervisor, your role is to oversee a dialogue between these\"\n",
    "    \" workers: {members}. Based on the user's request,\"\n",
    "    \" determine which worker should take the next action. Each worker is responsible for\"\n",
    "    \" executing a specific task and reporting back their findings and progress. Once all tasks are complete,\"\n",
    "    \" indicate with 'FINISH'.\"\n",
    ")\n",
    "\n",
    "options = [\"FINISH\"] + members\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"next\": {\"title\": \"Next\", \"anyOf\": [{\"enum\": options}] }},\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    (\"system\", \"Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}\"),\n",
    "]).partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "supervisor_chain = (prompt | llm.bind_functions(functions=[function_def], function_call=\"route\") | JsonOutputFunctionsParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Search Agent\n",
    "search_agent = create_agent(llm, tools, \"You are a web searcher. Search the internet for information.\")\n",
    "search_node = functools.partial(agent_node, agent=search_agent, name=\"Web_Searcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Web Insight Researcher Agent\n",
    "insights_research_agent = create_agent(llm, tools, \n",
    "        \"\"\"You are a Insight Researcher. Do step by step. \n",
    "        Based on the provided content first identify the list of topics,\n",
    "        then search internet for each topic one by one\n",
    "        and finally find insights for each topic one by one.\n",
    "        Include the insights and sources in the final response\n",
    "        \"\"\")\n",
    "\n",
    "insights_research_node = functools.partial(agent_node, agent=insights_research_agent, name=\"Insight_Researcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Agent State\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next: str\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Web_Searcher\", search_node)\n",
    "workflow.add_node(\"Insight_Researcher\", insights_research_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define edges\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# Compile the workflow graph\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Web_Searcher'}}\n",
      "----\n",
      "{'Web_Searcher': {'messages': [HumanMessage(content=\"### Summary of Veal Ear Tag Technology Trends in 2024:\\n\\n1. **SenseHub Feedlot Technology:** Merck's SenseHub Feedlot technology tracks animal health via an ear tag, collecting information like temperature and behavior to diagnose illness faster in cattle. It aims to increase the number of cattle reaching processing plants by reducing morbidity and mortality rates.\\n\\n2. **SenseHub Cow Calf Tags:** These tags focus on cow health and cycle monitoring, helping to increase reproduction rates within cow/calf herds by monitoring temperatures and movements to identify cows in heat or potentially ill cows. They aid in artificial insemination and embryo transfer to enhance conception rates.\\n\\n3. **Activity Monitoring Systems:** Transferred from the dairy industry, these systems track rumination and eating time in cattle, benefiting cow-calf producers and feedlots. They detect cows in heat after timed AI protocols and have shown good results in detecting animals in estrus with high accuracy.\\n\\n4. **RFID Ear Tags:** These tags are essential for smartphone-based record-keeping systems and precision feeding systems in beef operations. They enable automatic recording of weights, vaccinations, treatments, and breeding activities, facilitating easier record-keeping and individualized feeding based on nutritional needs.\\n\\n5. **Precision Livestock Farming Tools:** Various technologies are available to improve cattle management and forage/pasture management, enhancing overall farm operations and productivity.\\n\\n### Insights for Each Topic:\\n\\n1. **SenseHub Feedlot Technology:**\\n   - Enables faster diagnosis of cattle illnesses.\\n   - Improves cattle health monitoring and reduces morbidity and mortality rates.\\n   - Enhances decision-making for producers and pen riders.\\n\\n2. **SenseHub Cow Calf Tags:**\\n   - Improves reproductive rates within cow/calf herds.\\n   - Enhances artificial insemination and embryo transfer success.\\n   - Facilitates early detection of health issues and heat cycles in cows.\\n\\n3. **Activity Monitoring Systems:**\\n   - Benefits cow-calf producers and feedlots by tracking cattle activities.\\n   - Improves heat detection accuracy and reproductive efficiency.\\n   - Supports better management practices for cattle health and well-being.\\n\\n4. **RFID Ear Tags:**\\n   - Essential for record-keeping and precision feeding systems.\\n   - Enables automated data collection and individualized feeding.\\n   - Enhances farm management efficiency and productivity.\\n\\n5. **Precision Livestock Farming Tools:**\\n   - Offer a range of technologies for cattle and forage management.\\n   - Improve operational practices and decision-making.\\n   - Enhance overall farm performance and animal welfare.\\n\\nThese insights highlight the advancements in veal ear tag technology in 2024 and the benefits they offer to cattle producers in enhancing animal health, reproduction, and farm management practices.\", name='Web_Searcher')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'Insight_Researcher'}}\n",
      "----\n",
      "{'Insight_Researcher': {'messages': [HumanMessage(content=\"The Wyoming Livestock Roundup reported on the emerging technology in the cattle industry, specifically focusing on the SenseHub Cow Calf tags. These tags monitor cows' temperature and movement to enhance cow health and cycle monitoring in cow/calf herds. The information gathered is sent to a user-friendly interface that notifies producers of cows in heat or potentially ill cows, aiding in artificial insemination and embryo transfer to improve conception rates.\\n\\nStudies have shown that using SenseHub in feedyards has increased the number of cattle reaching processing plants by reducing morbidity and mortality rates. The goal of SenseHub Cow Calf tags is to increase reproduction rates within cow/calf herds by accurately monitoring cows' health and heat cycles, ultimately enhancing reproductive efficiency and overall herd management.\\n\\nThis technology is crucial for producers who rely on artificial insemination and embryo transfer, as it helps in identifying cows in heat, including those with silent heat signs that may be missed through visual observation alone. By leveraging temperature indicators, producers can enhance their ability to detect cows in heat and optimize conception rates for improved breeding outcomes.\\n\\nThe insights from this article emphasize the significant impact of SenseHub Cow Calf tags on improving reproductive rates, enhancing artificial insemination success, and facilitating early detection of health issues and heat cycles in cows within cow/calf herds. This technology plays a vital role in optimizing herd management practices and boosting overall reproductive efficiency.\", name='Insight_Researcher')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Run the graph\n",
    "for s in graph.stream({\n",
    "    \"messages\": [HumanMessage(content=\"\"\"Search for the latest veal ear tag technology trends in 2024,\n",
    "            summarize the content. After summarise pass it on to insight researcher\n",
    "            to provide insights for each topic\"\"\")]\n",
    "}):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_response = graph.invoke({\n",
    "#     \"messages\": [HumanMessage(\n",
    "#         content=\"\"\"Search for the latest veal ear tag technology trends in 2024,\n",
    "#                 summarize the content\n",
    "#                 and provide insights for each topic.\"\"\")]\n",
    "# })\n",
    "\n",
    "# print(final_response['messages'][1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
