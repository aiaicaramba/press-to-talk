{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike LangGraph (Re)Search Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq langchain langgraph langchain_openai langchainhub langsmith duckduckgo-search beautifulsoup4 gradio ipywidgets tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Commented out if set in .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'lm-studio' # Set to 'lm-studio' for local LMStudio inference\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = ''\n",
    "\n",
    "# Set LangSmith tracing and project name reference variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true' \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = '20240422-Spike LangGraph Local LLM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import functools, operator, requests, os, json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "#llm = ChatOpenAI(model=\"NA\", base_url=\"http://192.168.50.116:8000/v1\") # Use for local inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internet Search Tool (TavilySearch)\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "internet_search = TavilySearchResults(max_results=5)\n",
    "\n",
    "# Process Content Tool (BS4)\n",
    "@tool(\"process_content\", return_direct=False)\n",
    "def process_content(url: str) -> str:\n",
    "    \"\"\"Processes content from a webpage.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Tool set\n",
    "tools = [internet_search, process_content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for creating agents\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor\n",
    "\n",
    "# Define agent nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent Supervisor\n",
    "members = [\"Web_Searcher\", \"Insight_Researcher\"]\n",
    "system_prompt = (\n",
    "    \"As a supervisor, your role is to oversee a dialogue between these\"\n",
    "    \" workers: {members}. Based on the user's request,\"\n",
    "    \" determine which worker should take the next action. Each worker is responsible for\"\n",
    "    \" executing a specific task and reporting back their findings and progress. Once all tasks are complete,\"\n",
    "    \" indicate with 'FINISH'.\"\n",
    ")\n",
    "\n",
    "options = [\"FINISH\"] + members\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"next\": {\"title\": \"Next\", \"anyOf\": [{\"enum\": options}] }},\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    (\"system\", \"Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}\"),\n",
    "]).partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "supervisor_chain = (prompt | llm.bind_functions(functions=[function_def], function_call=\"route\") | JsonOutputFunctionsParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Search Agent\n",
    "search_agent = create_agent(llm, tools, \"You are a web searcher. Search the internet for information.\")\n",
    "search_node = functools.partial(agent_node, agent=search_agent, name=\"Web_Searcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Web Insight Researcher Agent\n",
    "insights_research_agent = create_agent(llm, tools, \n",
    "        \"\"\"You are a Insight Researcher. Do step by step. \n",
    "        Based on the provided content first identify the list of topics,\n",
    "        then search internet for each topic one by one\n",
    "        and finally find insights for each topic one by one.\n",
    "        Include the insights and sources in the final response\n",
    "        \"\"\")\n",
    "\n",
    "insights_research_node = functools.partial(agent_node, agent=insights_research_agent, name=\"Insight_Researcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Agent State\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next: str\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Web_Searcher\", search_node)\n",
    "workflow.add_node(\"Insight_Researcher\", insights_research_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define edges\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# Compile the workflow graph\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the graph\n",
    "for s in graph.stream({\n",
    "    \"messages\": [HumanMessage(content=\"\"\"Search for the latest veal ear tag technology trends in 2024,\n",
    "            summarize the content. After summarise pass it on to insight researcher\n",
    "            to provide insights for each topic\"\"\")]\n",
    "}):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_response = graph.invoke({\n",
    "#     \"messages\": [HumanMessage(\n",
    "#         content=\"\"\"Search for the latest veal ear tag technology trends in 2024,\n",
    "#                 summarize the content\n",
    "#                 and provide insights for each topic.\"\"\")]\n",
    "# })\n",
    "\n",
    "# print(final_response['messages'][1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
