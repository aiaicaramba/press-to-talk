{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain_openai langsmith pandas langchain_experimental langgraph langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api keys\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Optional, add tracing in LangSmith\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent\n",
    "\n",
    "The `create_agent` function is used to create an AI agent with a specific language model and a set of tools. The function takes three arguments: `llm`, `tools`, and `system_message`.\n",
    "\n",
    "The `llm` argument is the language model that will be used by the agent. This model is responsible for generating the agent's responses.\n",
    "\n",
    "The `tools` argument is a list of tools that the agent has access to. These tools are transformed into functions that the language model can use, by using a list comprehension to apply the `format_tool_to_openai_function` function to each tool.\n",
    "\n",
    "The `system_message` argument is a string that will be included in the prompt that the agent uses to generate its responses.\n",
    "\n",
    "The function first creates a `ChatPromptTemplate` object, which is a template for the prompts that the agent will use. This template includes a system message that describes the agent's role and instructions, as well as a placeholder for additional messages.\n",
    "\n",
    "The `system_message` and `tool_names` are then added to the prompt using the `partial` method, which fills in parts of the template.\n",
    "\n",
    "Finally, the function binds the functions derived from the tools to the language model and combines this with the prompt using the `|` operator. This creates a pipeline where the prompt is used to generate a response from the language model, and the functions are used to process the response. The resulting agent is then returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"\n",
    "    Create an agent.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model to bind the functions to.\n",
    "        tools: The list of tools available to the agent.\n",
    "        system_message: The system message to be included in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        The created agent.\n",
    "\n",
    "    \"\"\"\n",
    "    functions = [format_tool_to_openai_function(t) for t in tools] # format the tools to OpenAI functions \n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    prompt = prompt.partial(system_message=system_message) # add the system message to the prompt\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools])) # add the tool names to the prompt\n",
    "    \n",
    "    return prompt | llm.bind_functions(functions) # output of the prompt is used as the input for the llm with its bound functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import Annotated\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5) # create a Tavily search tool to search for results\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "repl = PythonREPL() # create a Python REPL tool to execute Python code\n",
    "\n",
    "\n",
    "@tool # add the tool decorator to the function\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute in the REPL.\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Use this function to execute Python code.\n",
    "    \n",
    "    Args:\n",
    "        code (str): The Python code to execute.\n",
    "        \n",
    "    Returns:\n",
    "        str: A string indicating the result of the execution and the stdout.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    return f\"Succesfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Succesfully executed:\n",
       "```python\n",
       "print('Hello, World!')\n",
       "```\n",
       "Stdout: Hello, World!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def print_md(md):\n",
    "    display(Markdown(md))\n",
    "\n",
    "print_md(python_repl(\"print('Hello, World!')\")) # test the Python REPL tool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
